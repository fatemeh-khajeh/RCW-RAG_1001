Hello everyone, now welcome to my Code to Care video series. What I'm doing is I'm rotating through three different types of topics, educational topics, use case topics, and then kind of bias, ethics, safety topics. So now on the education rotation. And today what I wanted to talk about is what is retrieval, augmented generation or reg. And you may think that I'm going into some kind of nook and cranny of the AI field, but this is a very important and popular kind of solution pattern that I see being used over and over and over again for how to leverage large language models. So I thought I would explain it to you. And the thing that this is used for is basically systems that leverage large language models, but on your own content. So let me describe that. If you think of like the chat GPT experience, and if you think about that relative to like the search engine experience that we had before, if you ask a question like, I don't know what color is the sky or how do I fix this plumbing issue or something like that, a search engine would go out or appear to go out, search the internet, find relevant content, and then just list that content for you. List those links and then you as a user would need to click on the links that seem, seem right, read it, digest it, and figure out the answer to your question. What a large language model does is it seems to do that first part, meaning leverage the content on the whole internet. But instead of just listing that content, it sort of digest ship, digest it, combines it, assembles it together, and answers your question, sort of generates an answer. So it's a whole lot better. I mean, search engines haven't been great, but this is taking the whole experience to another level. And it is a question and answering. Now you can also give it instructions like write me this document or write me a lesson plan to teach geometry to seventh graders, and it will do something similar. It will kind of assemble content that it has seen that talks about geometry or seventh graders or how to do lesson plans or whatever, pulls that together, assembles it and then writes out a lesson plan. So it's a much better experience than just taking the raw content from the internet, but it really creates something new from that. Now let's say you want that same experience, but on your own content. So it might be a chatbot on your website or you might have a library of PDF documents that this documentation for one of your products. And instead of just linking the user to pair of sections of the documentation, you want to actually answer their question. It might be your service ticketing system. So when a new issue comes in, you could say, how would I resolve this issue? And it can assemble past similar issues and then come up with a new solution based on that. So this is an incredible experience that these large language models offer, but how can you create that experience on your own content that might not be available to the internet or available to these large language models? Well the solution to this is this rag architecture, this retrieval augmented generation architecture. So now I'm going to do my best to explain that to you. So let's say you have a user and I'm going to use the example of a patient chatbot and the content source is going to be the content from your website, let's say, or could be content from PDF documents or whatever, but you want this to be the content to answer the patient's questions. So if the patient has a question like, how do I prepare for my knee surgery? Instead of just going to chat to your BT and getting a generic answer, you'd like to provide an answer that's from your health system. Or a question like, do you have parking? You'd like to provide an answer for your health system. For your, the office where the patient is seen. So that's a scenario that I'd like to do. So the patient has a question and I'm going to do, do you have parking? And parking. You can imagine that question being bundled up into a prompt. What's called a prompt and I'll describe this more later. There is the question. That prompt is sent to a large language model and that large language model will come up with a response to that question. Okay. Now if you just want to use chat to BT, let's say, or some other LOM without any extra content, you could just use this flow. How do I prepare for my knee surgery or do you have parking? Let that into a prompt, send that to the large language model and get a response back. But what we want to do is enhance this experience with our own content. So let's say here is your content source. And again, this might be all the content of your website or PDF documents or internal ticketing system or databases or that sort of thing. And what you'd like to do is something called the prompt before the prompt. So in these systems, you don't just send the user question to the large language model. You usually have some level of instructions. So the instructions might be you are a contact center specialist working for a hospital answering patient questions that come in over the internet. Please be nice to the patients and responsive and folksy because that fits with our brand. Or some instructions like that are sometimes sent with the prompt. And then additionally, you want to provide the information that the LOM needs to answer the question. So what you'd ideally like is information from your website to be included here. And that to be sent to the LOM as well. So the full prompt might be your instructions. It might be something like please use this content in order to answer the patient question at the end. And then you put in a bunch of information about parking or about knee surgery or whenever the patient asked, you put that in the prompt before the prompt. Then you have the question. Then you send that whole package to the LOM and the LOM will give a great response based on your content. Okay? With me so far. So this notion is the prompt before the prompt. And that's why prompt engineering and these types of things are big field right now because you can really hone the systems by doing a better and better job with the actual prompt before the prompt in the style. Now the last trick here is your website or your content is huge and it talks about all kinds of topics beyond parking and beyond knee surgery. So you really want to somehow pull out only the parts of your content that are relevant to the patient's question. So this is another tricky part of this whole rag architecture. And the way that works is that you take all your content and you break it into chunks or these systems will break it into chunks. So chunks might be a paragraph of content or a couple paragraphs, a page, something like that. And then those chunks are sent to a large language model, could be the same one or a different one. And they are turned into a vector. And so each each paragraph or each chunk will have a vector which is just a series of numbers. And that series of numbers, you can think of it as the numeric representation of the essence of that paragraph. And what's different about these numbers, they're not random numbers, but paragraphs to talk about a similar topic have close by numbers. They almost have the same vectors. So in addition to the, it's a numericized version of the paragraph, but it's such that similar paragraphs on similar topics will have similar vectors, will have similar numbers. So that means that what happens is when a user will ask a question like do you have parking let's say. Then that is also sent to the LOM in real time right after the user asked the question. That comes up with the vector as well. You could think of that as the question vector. And then what happens is we do a mathematical comparison real quick between the vector of the question and then the vectors of your content and pick like the top five documents that are closest to this question. So do you have parking will be a vector. Then you have all your content and it's going to try and find the five documents that talk the most about parking basically. And so it'll find us, I don't know what that is, it'll find those documents let's say from these, it'll grab the paragraphs associated with those documents and it'll use that here. So those will be the subset of your content basically that is used as part of the prompt before the prompt. Okay. So this whole concept is kind of vectorizing your content. Typically that then are stored in something called a vector database, which is basically a representation of your content in this numeric form. And then this system that you build this rag system will take the question, find retrieve the most relevant content, make that as part of the prompt before the prompt, send that to the LOM and then you get a good response back actually. So it's a little bit confusing, but it's actually not that confusing. I just made it more confusing by this horrible drawing. But this whole thing is what is called rag retrieval. So you're retrieving the relevant documents from your content. You're augmenting the generation process. So you're augmenting the LLOM's ability to do generative AI based on the documents that you retrieve. So that's why it's retrieval augmenting generation. Okay. So I hope that made sense. Like I said, this is a very popular solution pattern that I'm seeing over and over again. In fact, the majority of LLOM projects that I see are this kind of thing. Using my content, packaging that up with an LLOM system to create a kind of chat GPT-like experience for my employees or for my customers, for my users, that kind of thing. And it works extremely well. That's why it's so popular. So I hope that was interesting and educational and made sense. If you have any questions, please leave them for me as part of the comments. Thank you very much.