{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RAG Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the Model  /  Configurer le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Elon Musk is a billionaire entrepreneur and CEO of multiple companies, including Tesla, SpaceX, Neuralink, and The Boring Company. He is known for his work in the fields of electric vehicles, space exploration, and renewable energy. Musk is also a prominent figure in popular culture and is often referred to as a visionary and innovator.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 68, 'prompt_tokens': 12, 'total_tokens': 80, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-99226f9b-5f0b-4db4-8c96-64b6fa152987-0', usage_metadata={'input_tokens': 12, 'output_tokens': 68, 'total_tokens': 80})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "# Test the model\n",
    "model.invoke(\"Who is Elon Musk?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Elon Musk is a billionaire entrepreneur and CEO of multiple companies, including Tesla, SpaceX, Neuralink, and The Boring Company. He is known for his work in the fields of electric vehicles, space exploration, and renewable energy. Musk is also a prominent figure in popular culture and is often referred to as a visionary and innovator.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use StrOutputParser to extract the answer as a string\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()\n",
    "chain = model | parser\n",
    "chain.invoke(\"Who is Elon Musk?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduce Prompts Template\n",
    "## Introduire le Prompts Template -- Modèle d'invite de présentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't answer the question, \n",
    "reply \"My apologies, but I have no clue\".env\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chain the prompt with the model and output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | parser "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate / combining Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a new prompt template for translating the output into Spanish/ French\n",
    "translation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the {answer} to {language}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spanish: Sarra tiene una hermana, Cerine.\\nPortuguese: Sarra tem uma irmã, Cerine.\\nFrench: Sarra a une sœur, Cerine.\\nFarsi: سارا یک خواهر دارد، سرین.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a new translation chain that combines the first chain with the second one (translation prompt)\n",
    "from operator import itemgetter\n",
    "translation_chain = (\n",
    "    {\"answer\": chain, \"language\": itemgetter(\"language\")} | translation_prompt | model | parser\n",
    ")\n",
    "translation_chain.invoke(\n",
    "    {\n",
    "        \"context\": \"Sarra's sister is Cerine. She does not have any more siblings.\",\n",
    "        \"question\": \"How many sisters does Sarra have?\",\n",
    "        \"language\": [\"Spanish\", \"Portuguese\", \"French\", \"Farsi\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribing the YouTube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to send a context to the model from YouTUBE. Let's OpenAI Whisper\n",
    "import tempfile\n",
    "import whisper\n",
    "from pytubefix import YouTube\n",
    "\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=u47GtXwePms\"\n",
    "# Check if file not exist then create ...\n",
    "if not os.path.exists(\"transcription.txt\"):\n",
    "    youtube = YouTube(YOUTUBE_VIDEO)\n",
    "    audio = youtube.streams.filter(only_audio=True).first()\n",
    "    \n",
    "    # Let's loas the base model. Not accurate\n",
    "    whisper_model = whisper.load_model(\"base\")\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        file = audio.download(output_path=tmpdir)\n",
    "        transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
    "        \n",
    "        with open(\"transcription.txt\", \"w\") as file:\n",
    "            file.write(transcription)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### display the first few characters to ensure all is working / afficher les premiers caractères pour s'assurer que tout fonctionne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello everyone, now welcome to my Code to Care video series. What I'm doing is I'm rotating through three different type\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"transcription.txt\") as file:\n",
    "    transcription = file.read()\n",
    "transcription[: 120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the entire transcription as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some challenges of LLM include the need for prompt engineering to ensure accurate responses, the difficulty of extracting relevant information from large amounts of content, and the potential bias in the generated responses.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = chain.invoke({\n",
    "        \"context\": transcription,\n",
    "        \"question\": \"What are some challenges of LLM? \"\n",
    "    })\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the transcript in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'transcription.txt'}, page_content=\"Hello everyone, now welcome to my Code to Care video series. What I'm doing is I'm rotating through three different types of topics, educational topics, use case topics, and then kind of bias, ethics, safety topics. So now on the education rotation. And today what I wanted to talk about is what is retrieval, augmented generation or reg. And you may think that I'm going into some kind of nook and cranny of the AI field, but this is a very important and popular kind of solution pattern that I see being used over and over and over again for how to leverage large language models. So I thought I would explain it to you. And the thing that this is used for is basically systems that leverage large language models, but on your own content. So let me describe that. If you think of like the chat GPT experience, and if you think about that relative to like the search engine experience that we had before, if you ask a question like, I don't know what color is the sky or how do I fix this plumbing issue or something like that, a search engine would go out or appear to go out, search the internet, find relevant content, and then just list that content for you. List those links and then you as a user would need to click on the links that seem, seem right, read it, digest it, and figure out the answer to your question. What a large language model does is it seems to do that first part, meaning leverage the content on the whole internet. But instead of just listing that content, it sort of digest ship, digest it, combines it, assembles it together, and answers your question, sort of generates an answer. So it's a whole lot better. I mean, search engines haven't been great, but this is taking the whole experience to another level. And it is a question and answering. Now you can also give it instructions like write me this document or write me a lesson plan to teach geometry to seventh graders, and it will do something similar. It will kind of assemble content that it has seen that talks about geometry or seventh graders or how to do lesson plans or whatever, pulls that together, assembles it and then writes out a lesson plan. So it's a much better experience than just taking the raw content from the internet, but it really creates something new from that. Now let's say you want that same experience, but on your own content. So it might be a chatbot on your website or you might have a library of PDF documents that this documentation for one of your products. And instead of just linking the user to pair of sections of the documentation, you want to actually answer their question. It might be your service ticketing system. So when a new issue comes in, you could say, how would I resolve this issue? And it can assemble past similar issues and then come up with a new solution based on that. So this is an incredible experience that these large language models offer, but how can you create that experience on your own content that might not be available to the internet or available to these large language models? Well the solution to this is this rag architecture, this retrieval augmented generation architecture. So now I'm going to do my best to explain that to you. So let's say you have a user and I'm going to use the example of a patient chatbot and the content source is going to be the content from your website, let's say, or could be content from PDF documents or whatever, but you want this to be the content to answer the patient's questions. So if the patient has a question like, how do I prepare for my knee surgery? Instead of just going to chat to your BT and getting a generic answer, you'd like to provide an answer that's from your health system. Or a question like, do you have parking? You'd like to provide an answer for your health system. For your, the office where the patient is seen. So that's a scenario that I'd like to do. So the patient has a question and I'm going to do, do you have parking? And parking. You can imagine that question being bundled up into a prompt. What's called a prompt and I'll describe this more later. There is the question. That prompt is sent to a large language model and that large language model will come up with a response to that question. Okay. Now if you just want to use chat to BT, let's say, or some other LOM without any extra content, you could just use this flow. How do I prepare for my knee surgery or do you have parking? Let that into a prompt, send that to the large language model and get a response back. But what we want to do is enhance this experience with our own content. So let's say here is your content source. And again, this might be all the content of your website or PDF documents or internal ticketing system or databases or that sort of thing. And what you'd like to do is something called the prompt before the prompt. So in these systems, you don't just send the user question to the large language model. You usually have some level of instructions. So the instructions might be you are a contact center specialist working for a hospital answering patient questions that come in over the internet. Please be nice to the patients and responsive and folksy because that fits with our brand. Or some instructions like that are sometimes sent with the prompt. And then additionally, you want to provide the information that the LOM needs to answer the question. So what you'd ideally like is information from your website to be included here. And that to be sent to the LOM as well. So the full prompt might be your instructions. It might be something like please use this content in order to answer the patient question at the end. And then you put in a bunch of information about parking or about knee surgery or whenever the patient asked, you put that in the prompt before the prompt. Then you have the question. Then you send that whole package to the LOM and the LOM will give a great response based on your content. Okay? With me so far. So this notion is the prompt before the prompt. And that's why prompt engineering and these types of things are big field right now because you can really hone the systems by doing a better and better job with the actual prompt before the prompt in the style. Now the last trick here is your website or your content is huge and it talks about all kinds of topics beyond parking and beyond knee surgery. So you really want to somehow pull out only the parts of your content that are relevant to the patient's question. So this is another tricky part of this whole rag architecture. And the way that works is that you take all your content and you break it into chunks or these systems will break it into chunks. So chunks might be a paragraph of content or a couple paragraphs, a page, something like that. And then those chunks are sent to a large language model, could be the same one or a different one. And they are turned into a vector. And so each each paragraph or each chunk will have a vector which is just a series of numbers. And that series of numbers, you can think of it as the numeric representation of the essence of that paragraph. And what's different about these numbers, they're not random numbers, but paragraphs to talk about a similar topic have close by numbers. They almost have the same vectors. So in addition to the, it's a numericized version of the paragraph, but it's such that similar paragraphs on similar topics will have similar vectors, will have similar numbers. So that means that what happens is when a user will ask a question like do you have parking let's say. Then that is also sent to the LOM in real time right after the user asked the question. That comes up with the vector as well. You could think of that as the question vector. And then what happens is we do a mathematical comparison real quick between the vector of the question and then the vectors of your content and pick like the top five documents that are closest to this question. So do you have parking will be a vector. Then you have all your content and it's going to try and find the five documents that talk the most about parking basically. And so it'll find us, I don't know what that is, it'll find those documents let's say from these, it'll grab the paragraphs associated with those documents and it'll use that here. So those will be the subset of your content basically that is used as part of the prompt before the prompt. Okay. So this whole concept is kind of vectorizing your content. Typically that then are stored in something called a vector database, which is basically a representation of your content in this numeric form. And then this system that you build this rag system will take the question, find retrieve the most relevant content, make that as part of the prompt before the prompt, send that to the LOM and then you get a good response back actually. So it's a little bit confusing, but it's actually not that confusing. I just made it more confusing by this horrible drawing. But this whole thing is what is called rag retrieval. So you're retrieving the relevant documents from your content. You're augmenting the generation process. So you're augmenting the LLOM's ability to do generative AI based on the documents that you retrieve. So that's why it's retrieval augmenting generation. Okay. So I hope that made sense. Like I said, this is a very popular solution pattern that I'm seeing over and over again. In fact, the majority of LLOM projects that I see are this kind of thing. Using my content, packaging that up with an LLOM system to create a kind of chat GPT-like experience for my employees or for my customers, for my users, that kind of thing. And it works extremely well. That's why it's so popular. So I hope that was interesting and educational and made sense. If you have any questions, please leave them for me as part of the comments. Thank you very much.\")]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'transcription.txt'}, page_content=\"Hello everyone, now welcome to my Code to Care video series. What I'm doing is I'm rotating through three different types of topics, educational topics, use case topics, and then kind of bias, ethics, safety topics. So now on the education rotation. And today what I wanted to talk about is what is retrieval, augmented generation or reg. And you may think that I'm going into some kind of nook and cranny of the AI field, but this is a very important and popular kind of solution pattern that I see being used over and over and over again for how to leverage large language models. So I thought I would explain it to you. And the thing that this is used for is basically systems that leverage large language models, but on your own content. So let me describe that. If you think of like the chat GPT experience, and if you think about that relative to like the search engine experience that we had before, if you ask a question like, I don't know what color is the sky or how do I fix this plumbing\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"experience, and if you think about that relative to like the search engine experience that we had before, if you ask a question like, I don't know what color is the sky or how do I fix this plumbing issue or something like that, a search engine would go out or appear to go out, search the internet, find relevant content, and then just list that content for you. List those links and then you as a user would need to click on the links that seem, seem right, read it, digest it, and figure out the answer to your question. What a large language model does is it seems to do that first part, meaning leverage the content on the whole internet. But instead of just listing that content, it sort of digest ship, digest it, combines it, assembles it together, and answers your question, sort of generates an answer. So it's a whole lot better. I mean, search engines haven't been great, but this is taking the whole experience to another level. And it is a question and answering. Now you can also give\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"an answer. So it's a whole lot better. I mean, search engines haven't been great, but this is taking the whole experience to another level. And it is a question and answering. Now you can also give it instructions like write me this document or write me a lesson plan to teach geometry to seventh graders, and it will do something similar. It will kind of assemble content that it has seen that talks about geometry or seventh graders or how to do lesson plans or whatever, pulls that together, assembles it and then writes out a lesson plan. So it's a much better experience than just taking the raw content from the internet, but it really creates something new from that. Now let's say you want that same experience, but on your own content. So it might be a chatbot on your website or you might have a library of PDF documents that this documentation for one of your products. And instead of just linking the user to pair of sections of the documentation, you want to actually answer their\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"have a library of PDF documents that this documentation for one of your products. And instead of just linking the user to pair of sections of the documentation, you want to actually answer their question. It might be your service ticketing system. So when a new issue comes in, you could say, how would I resolve this issue? And it can assemble past similar issues and then come up with a new solution based on that. So this is an incredible experience that these large language models offer, but how can you create that experience on your own content that might not be available to the internet or available to these large language models? Well the solution to this is this rag architecture, this retrieval augmented generation architecture. So now I'm going to do my best to explain that to you. So let's say you have a user and I'm going to use the example of a patient chatbot and the content source is going to be the content from your website, let's say, or could be content from PDF documents\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"let's say you have a user and I'm going to use the example of a patient chatbot and the content source is going to be the content from your website, let's say, or could be content from PDF documents or whatever, but you want this to be the content to answer the patient's questions. So if the patient has a question like, how do I prepare for my knee surgery? Instead of just going to chat to your BT and getting a generic answer, you'd like to provide an answer that's from your health system. Or a question like, do you have parking? You'd like to provide an answer for your health system. For your, the office where the patient is seen. So that's a scenario that I'd like to do. So the patient has a question and I'm going to do, do you have parking? And parking. You can imagine that question being bundled up into a prompt. What's called a prompt and I'll describe this more later. There is the question. That prompt is sent to a large language model and that large language model will come up\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"bundled up into a prompt. What's called a prompt and I'll describe this more later. There is the question. That prompt is sent to a large language model and that large language model will come up with a response to that question. Okay. Now if you just want to use chat to BT, let's say, or some other LOM without any extra content, you could just use this flow. How do I prepare for my knee surgery or do you have parking? Let that into a prompt, send that to the large language model and get a response back. But what we want to do is enhance this experience with our own content. So let's say here is your content source. And again, this might be all the content of your website or PDF documents or internal ticketing system or databases or that sort of thing. And what you'd like to do is something called the prompt before the prompt. So in these systems, you don't just send the user question to the large language model. You usually have some level of instructions. So the instructions might\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"called the prompt before the prompt. So in these systems, you don't just send the user question to the large language model. You usually have some level of instructions. So the instructions might be you are a contact center specialist working for a hospital answering patient questions that come in over the internet. Please be nice to the patients and responsive and folksy because that fits with our brand. Or some instructions like that are sometimes sent with the prompt. And then additionally, you want to provide the information that the LOM needs to answer the question. So what you'd ideally like is information from your website to be included here. And that to be sent to the LOM as well. So the full prompt might be your instructions. It might be something like please use this content in order to answer the patient question at the end. And then you put in a bunch of information about parking or about knee surgery or whenever the patient asked, you put that in the prompt before the\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"in order to answer the patient question at the end. And then you put in a bunch of information about parking or about knee surgery or whenever the patient asked, you put that in the prompt before the prompt. Then you have the question. Then you send that whole package to the LOM and the LOM will give a great response based on your content. Okay? With me so far. So this notion is the prompt before the prompt. And that's why prompt engineering and these types of things are big field right now because you can really hone the systems by doing a better and better job with the actual prompt before the prompt in the style. Now the last trick here is your website or your content is huge and it talks about all kinds of topics beyond parking and beyond knee surgery. So you really want to somehow pull out only the parts of your content that are relevant to the patient's question. So this is another tricky part of this whole rag architecture. And the way that works is that you take all your\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"pull out only the parts of your content that are relevant to the patient's question. So this is another tricky part of this whole rag architecture. And the way that works is that you take all your content and you break it into chunks or these systems will break it into chunks. So chunks might be a paragraph of content or a couple paragraphs, a page, something like that. And then those chunks are sent to a large language model, could be the same one or a different one. And they are turned into a vector. And so each each paragraph or each chunk will have a vector which is just a series of numbers. And that series of numbers, you can think of it as the numeric representation of the essence of that paragraph. And what's different about these numbers, they're not random numbers, but paragraphs to talk about a similar topic have close by numbers. They almost have the same vectors. So in addition to the, it's a numericized version of the paragraph, but it's such that similar paragraphs on\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"to talk about a similar topic have close by numbers. They almost have the same vectors. So in addition to the, it's a numericized version of the paragraph, but it's such that similar paragraphs on similar topics will have similar vectors, will have similar numbers. So that means that what happens is when a user will ask a question like do you have parking let's say. Then that is also sent to the LOM in real time right after the user asked the question. That comes up with the vector as well. You could think of that as the question vector. And then what happens is we do a mathematical comparison real quick between the vector of the question and then the vectors of your content and pick like the top five documents that are closest to this question. So do you have parking will be a vector. Then you have all your content and it's going to try and find the five documents that talk the most about parking basically. And so it'll find us, I don't know what that is, it'll find those documents\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"you have all your content and it's going to try and find the five documents that talk the most about parking basically. And so it'll find us, I don't know what that is, it'll find those documents let's say from these, it'll grab the paragraphs associated with those documents and it'll use that here. So those will be the subset of your content basically that is used as part of the prompt before the prompt. Okay. So this whole concept is kind of vectorizing your content. Typically that then are stored in something called a vector database, which is basically a representation of your content in this numeric form. And then this system that you build this rag system will take the question, find retrieve the most relevant content, make that as part of the prompt before the prompt, send that to the LOM and then you get a good response back actually. So it's a little bit confusing, but it's actually not that confusing. I just made it more confusing by this horrible drawing. But this whole\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"to the LOM and then you get a good response back actually. So it's a little bit confusing, but it's actually not that confusing. I just made it more confusing by this horrible drawing. But this whole thing is what is called rag retrieval. So you're retrieving the relevant documents from your content. You're augmenting the generation process. So you're augmenting the LLOM's ability to do generative AI based on the documents that you retrieve. So that's why it's retrieval augmenting generation. Okay. So I hope that made sense. Like I said, this is a very popular solution pattern that I'm seeing over and over again. In fact, the majority of LLOM projects that I see are this kind of thing. Using my content, packaging that up with an LLOM system to create a kind of chat GPT-like experience for my employees or for my customers, for my users, that kind of thing. And it works extremely well. That's why it's so popular. So I hope that was interesting and educational and made sense. If you have\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"my employees or for my customers, for my users, that kind of thing. And it works extremely well. That's why it's so popular. So I hope that was interesting and educational and made sense. If you have any questions, please leave them for me as part of the comments. Thank you very much.\")]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Diviser le texte en morceaux (petites parties) avec un chevauchement pour préserver le contexte entre eux.\n",
    "# Split text into chunks (small parts) with some overlap to preserve context between them\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)  \n",
    "text_splitter.split_documents(text_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the 'most' relevant chunks / Trouver les morceaux « les plus » pertinents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded length: 1536\n",
      "[-0.012432429939508438, -0.016618860885500908, 0.007928845472633839, -0.017041731625795364, 0.011424585245549679, -0.004905312322080135, -0.03549303859472275, -0.004852453712373972, -0.03693079948425293, -0.023398904129862785, 0.009274987503886223, -0.00261651910841465, 0.0011558495461940765, 0.0021266925614327192, 0.002008640905842185, -0.00010714954260038212, 0.029093578457832336, 0.004961695522069931, -0.004193478263914585, 0.001330284052528441]\n"
     ]
    }
   ],
   "source": [
    "# Generate the embeddings for an arbitrary query\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "embedded_query = embeddings.embed_query(\"What is RAG?\")\n",
    "print(f\"Embedded length: {len(embedded_query)}\")\n",
    "print(embedded_query[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a Knowledge Base (KB) / Vector Store (VS)\n",
    "###### A Vector Store is a smart knowledge base that stores text as vectors (meaning), so you can do semantic search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Transcript into the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the chain using the correct vectorstore.add()\n",
    "'''\n",
    "A KB/VS is a database consisting of embeddings that specializes in fast similarity searches\n",
    "\n",
    "'''\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "documents = text_splitter.split_documents(text_documents)\n",
    "trans_vectorstore = DocArrayInMemorySearch.from_documents(documents=documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG stands for Retrieval Augmented Generation, which is an architecture used to retrieve relevant documents from content, augment the generation process, and improve the ability of large language models to generate AI based on the retrieved documents.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": trans_vectorstore.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    |parser\n",
    ")\n",
    "chain.invoke(\"What is RAG?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use Pinecone as Vector Store.\n",
    "Pinecone is the leading AI infrastructure for building accurate, secure, and scalable AI applications. Use Pinecone Database to store and search vector data at scale, or start with Pinecone Assistant to get a RAG application running in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking your documents, turning them into vector format (with meaning), \n",
    "# and storing them in Pinecone so your AI assistant can later search and answer questions based on them.\n",
    "\n",
    "# prendre vos documents, les transformer en format vectoriel (avec sens) \n",
    "# et les stocker dans Pinecone afin que votre assistant IA puisse ensuite rechercher et répondre à des questions en fonction de ceux-ci.\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "pinecone_index_name = \"rcw2025\"\n",
    "\n",
    "pinecone = PineconeVectorStore.from_documents(\n",
    "    documents=documents, embedding=embeddings, index_name=pinecone_index_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Pinecone as retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'transcription.txt'}, page_content=\"pull out only the parts of your content that are relevant to the patient's question. So this is another tricky part of this whole rag architecture. And the way that works is that you take all your content and you break it into chunks or these systems will break it into chunks. So chunks might be a paragraph of content or a couple paragraphs, a page, something like that. And then those chunks are sent to a large language model, could be the same one or a different one. And they are turned into a vector. And so each each paragraph or each chunk will have a vector which is just a series of numbers. And that series of numbers, you can think of it as the numeric representation of the essence of that paragraph. And what's different about these numbers, they're not random numbers, but paragraphs to talk about a similar topic have close by numbers. They almost have the same vectors. So in addition to the, it's a numericized version of the paragraph, but it's such that similar paragraphs on\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"pull out only the parts of your content that are relevant to the patient's question. So this is another tricky part of this whole rag architecture. And the way that works is that you take all your content and you break it into chunks or these systems will break it into chunks. So chunks might be a paragraph of content or a couple paragraphs, a page, something like that. And then those chunks are sent to a large language model, could be the same one or a different one. And they are turned into a vector. And so each each paragraph or each chunk will have a vector which is just a series of numbers. And that series of numbers, you can think of it as the numeric representation of the essence of that paragraph. And what's different about these numbers, they're not random numbers, but paragraphs to talk about a similar topic have close by numbers. They almost have the same vectors. So in addition to the, it's a numericized version of the paragraph, but it's such that similar paragraphs on\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"pull out only the parts of your content that are relevant to the patient's question. So this is another tricky part of this whole rag architecture. And the way that works is that you take all your content and you break it into chunks or these systems will break it into chunks. So chunks might be a paragraph of content or a couple paragraphs, a page, something like that. And then those chunks are sent to a large language model, could be the same one or a different one. And they are turned into a vector. And so each each paragraph or each chunk will have a vector which is just a series of numbers. And that series of numbers, you can think of it as the numeric representation of the essence of that paragraph. And what's different about these numbers, they're not random numbers, but paragraphs to talk about a similar topic have close by numbers. They almost have the same vectors. So in addition to the, it's a numericized version of the paragraph, but it's such that similar paragraphs on\")]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.similarity_search(\"How does RAG differ from Fine-Tune?\")[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Picone with the Chain / Combiner Picone avec la chaîne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG (Retrieval Augmented Generation) differs from Fine-Tune in that RAG architecture involves breaking content into chunks, converting them into vectors, and using large language models to generate responses based on similar topics. On the other hand, Fine-Tune typically involves fine-tuning a pre-trained language model on specific tasks or datasets to improve performance on those tasks.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "chain = (\n",
    "    {\"context\": pinecone.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "chain.invoke(\"How does RAG differ from Fine-Tune?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
